# PPO å¼ºåŒ–å­¦ä¹ é¡¹ç›®

è¿™æ˜¯ä¸€ä¸ªä½¿ç”¨ **Proximal Policy Optimization (PPO)** ç®—æ³•è®­ç»ƒæ™ºèƒ½ä½“çš„å¼ºåŒ–å­¦ä¹ é¡¹ç›®ï¼Œåœ¨ CartPole-v1 ç¯å¢ƒä¸­è¿›è¡Œè®­ç»ƒã€‚

## ğŸ“ é¡¹ç›®ç»“æ„

```
PPO/
â”œâ”€â”€ train.py           # è®­ç»ƒè„šæœ¬ï¼ˆä¸»ç¨‹åºï¼‰
â”œâ”€â”€ PPO_Model.py       # PPO ç®—æ³•å®ç°
â”œâ”€â”€ Net.py             # ç¥ç»ç½‘ç»œå®šä¹‰ï¼ˆActor å’Œ Criticï¼‰
â”œâ”€â”€ average.py         # æ»‘åŠ¨å¹³å‡å·¥å…·å‡½æ•°
â””â”€â”€ README.md          # é¡¹ç›®è¯´æ˜æ–‡æ¡£
```

## ğŸ¯ é¡¹ç›®ç®€ä»‹

æœ¬é¡¹ç›®å®ç°äº† PPO ç®—æ³•çš„æ ¸å¿ƒç»„ä»¶ï¼š
- **Actor ç½‘ç»œ**ï¼šç­–ç•¥ç½‘ç»œï¼Œè¾“å‡ºåŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ
- **Critic ç½‘ç»œ**ï¼šä»·å€¼ç½‘ç»œï¼Œä¼°è®¡çŠ¶æ€ä»·å€¼
- **GAE (Generalized Advantage Estimation)**ï¼šä¼˜åŠ¿å‡½æ•°ä¼°è®¡
- **Clipped Surrogate Objective**ï¼šPPO çš„æ ¸å¿ƒç›®æ ‡å‡½æ•°

## ğŸ”§ ç¯å¢ƒä¾èµ–

### Python ç‰ˆæœ¬
- Python 3.10+

### å¿…éœ€çš„åº“
```bash
torch>=2.0.0
numpy>=1.24.0
gymnasium>=0.29.0
matplotlib>=3.7.0
tqdm>=4.65.0
```

## ğŸ“¦ å®‰è£…è¯´æ˜

### 1. åˆ›å»º Conda ç¯å¢ƒï¼ˆæ¨èï¼‰
```bash
conda create -n ppo_env python=3.10
conda activate ppo_env
```

### 2. å®‰è£…ä¾èµ–
```bash
pip install torch numpy gymnasium matplotlib tqdm
```

æˆ–è€…ä½¿ç”¨ requirements.txtï¼š
```bash
pip install -r requirements.txt
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### è¿è¡Œè®­ç»ƒ
```bash
python train.py
```

è®­ç»ƒå®Œæˆåä¼šè‡ªåŠ¨æ˜¾ç¤ºå›æŠ¥æ›²çº¿å›¾ã€‚

### è®­ç»ƒå‚æ•°è¯´æ˜

åœ¨ `train.py` ä¸­å¯ä»¥è°ƒæ•´ä»¥ä¸‹è¶…å‚æ•°ï¼š

```python
actor_lr = 1e-3        # Actor ç½‘ç»œå­¦ä¹ ç‡
critic_lr = 1e-2       # Critic ç½‘ç»œå­¦ä¹ ç‡
num_episodes = 300     # è®­ç»ƒæ€»å›åˆæ•°
hidden_dim = 128       # éšè—å±‚ç»´åº¦
gamma = 0.98           # æŠ˜æ‰£å› å­
lambda_ = 0.95         # GAE çš„ Î» å‚æ•°
epochs = 5             # æ¯æ¬¡æ›´æ–°çš„è®­ç»ƒè½®æ•°
eps = 0.2              # PPO å‰ªåˆ‡èŒƒå›´ Îµ
```

## ğŸ“Š ä»£ç è¯´æ˜

### 1. `train.py` - è®­ç»ƒä¸»è„šæœ¬

è´Ÿè´£ï¼š
- ç¯å¢ƒåˆå§‹åŒ–
- æ™ºèƒ½ä½“è®­ç»ƒå¾ªç¯
- æ•°æ®æ”¶é›†å’Œå›æŠ¥è®°å½•
- ç»“æœå¯è§†åŒ–

### 2. `PPO_Model.py` - PPO ç®—æ³•æ ¸å¿ƒ

å®ç°äº†ï¼š
- `take_action()`: æ ¹æ®å½“å‰ç­–ç•¥é‡‡æ ·åŠ¨ä½œ
- `update()`: PPO çš„ç­–ç•¥æ›´æ–°è¿‡ç¨‹
  - è®¡ç®— TD ç›®æ ‡å’Œ TD è¯¯å·®
  - ä½¿ç”¨ GAE è®¡ç®—ä¼˜åŠ¿å‡½æ•°
  - Clipped surrogate objective ä¼˜åŒ–

**å…³é”®å…¬å¼**ï¼š

PPO çš„ç›®æ ‡å‡½æ•°ï¼š
```
L^CLIP(Î¸) = E[min(r(Î¸)Â·A, clip(r(Î¸), 1-Îµ, 1+Îµ)Â·A)]
```

å…¶ä¸­ï¼š
- `r(Î¸) = Ï€_Î¸(a|s) / Ï€_Î¸_old(a|s)` æ˜¯æ–°æ—§ç­–ç•¥çš„æ¦‚ç‡æ¯”
- `A` æ˜¯ä¼˜åŠ¿å‡½æ•°
- `Îµ` æ˜¯å‰ªåˆ‡èŒƒå›´ï¼ˆé»˜è®¤ 0.2ï¼‰

### 3. `Net.py` - ç¥ç»ç½‘ç»œå®šä¹‰

åŒ…å«ï¼š
- `PolicyNet`: Actor ç½‘ç»œï¼ˆçŠ¶æ€ â†’ åŠ¨ä½œæ¦‚ç‡ï¼‰
- `ValueNet`: Critic ç½‘ç»œï¼ˆçŠ¶æ€ â†’ çŠ¶æ€ä»·å€¼ï¼‰
- `compute_advantage()`: GAE ä¼˜åŠ¿å‡½æ•°è®¡ç®—

### 4. `average.py` - å·¥å…·å‡½æ•°

å®ç°æ»‘åŠ¨çª—å£å¹³å‡ï¼Œç”¨äºå¹³æ»‘è®­ç»ƒæ›²çº¿ã€‚

## ğŸ“ˆ è®­ç»ƒæ•ˆæœ

åœ¨ CartPole-v1 ç¯å¢ƒä¸­ï¼š
- **æœ€å¤§å›æŠ¥**: 500ï¼ˆç¯å¢ƒé™åˆ¶ï¼‰
- **æ”¶æ•›é€Ÿåº¦**: é€šå¸¸åœ¨ 100-200 ä¸ª episode åè¾¾åˆ°ç¨³å®š
- **é¢„æœŸæ€§èƒ½**: è®­ç»ƒè‰¯å¥½çš„æ™ºèƒ½ä½“å¯ä»¥ç¨³å®šè·å¾— 450-500 çš„å›æŠ¥

## ğŸ® ç¯å¢ƒè¯´æ˜

### CartPole-v1
- **è§‚å¯Ÿç©ºé—´**: 4 ç»´è¿ç»­å‘é‡ï¼ˆä½ç½®ã€é€Ÿåº¦ã€è§’åº¦ã€è§’é€Ÿåº¦ï¼‰
- **åŠ¨ä½œç©ºé—´**: 2 ä¸ªç¦»æ•£åŠ¨ä½œï¼ˆå·¦æ¨ã€å³æ¨ï¼‰
- **å¥–åŠ±**: æ¯ä¸ªæ—¶é—´æ­¥ +1
- **ç»ˆæ­¢æ¡ä»¶**: 
  - æ†å­å€¾æ–œè¶…è¿‡ Â±12Â°
  - å°è½¦ç¦»å¼€è¾¹ç•Œï¼ˆÂ±2.4ï¼‰
  - è¾¾åˆ° 500 æ­¥ï¼ˆæˆªæ–­ï¼‰

## ğŸ”„ åˆ‡æ¢åˆ°å…¶ä»–ç¯å¢ƒ

ä¿®æ”¹ `train.py` ä¸­çš„ç¯å¢ƒåç§°ï¼š

```python
# å…¶ä»–å¯ç”¨ç¯å¢ƒ
env_name = "Acrobot-v1"        # åŒè¿æ†å€’ç«‹æ‘†
env_name = "LunarLander-v2"    # æœˆçƒç™»é™†å™¨
env_name = "MountainCar-v0"    # çˆ¬å±±è½¦
```

**æ³¨æ„**: ä¸åŒç¯å¢ƒå¯èƒ½éœ€è¦è°ƒæ•´è¶…å‚æ•°å’Œç½‘ç»œç»“æ„ã€‚

## ğŸ› å¸¸è§é—®é¢˜

### 1. è®­ç»ƒä¸æ”¶æ•›æˆ–å›æŠ¥ä¸‹é™

**åŸå› **: `actor_loss` ç¬¦å·å¯èƒ½é”™è¯¯ã€‚

**è§£å†³æ–¹æ¡ˆ**: åœ¨ `PPO_Model.py` ç¬¬ 60 è¡Œç¡®ä¿æœ‰è´Ÿå·ï¼š
```python
actor_loss = -th.mean(th.min(surr1, surr2))  # æ³¨æ„è´Ÿå·ï¼
```

### 2. `ValueError: operands could not be broadcast`

**åŸå› **: `moving_average` å‡½æ•°çš„ `window_size` ä¸åŒ¹é…ã€‚

**è§£å†³æ–¹æ¡ˆ**: åœ¨ `train.py` ä¸­è°ƒæ•´çª—å£å¤§å°ï¼š
```python
mv_return_list = moving_average(return_list, 9)  # ä½¿ç”¨å¥‡æ•°çª—å£
```

### 3. `AssertionError: XX invalid`

**åŸå› **: PPO æ„é€ å‡½æ•°å‚æ•°é¡ºåºé”™è¯¯ã€‚

**è§£å†³æ–¹æ¡ˆ**: ç¡®ä¿å‚æ•°é¡ºåºæ­£ç¡®ï¼š
```python
agent = PPO(state_dim, hidden_dim, action_dim, 
            actor_lr, critic_lr, lambda_, epochs, eps, gamma, device)
```

### 4. `UserWarning: Creating a tensor from a list...`

**åŸå› **: ä» Python åˆ—è¡¨ç›´æ¥åˆ›å»ºå¼ é‡æ•ˆç‡ä½ã€‚

**è§£å†³æ–¹æ¡ˆ**: å·²åœ¨ `PPO_Model.py` ä¸­ä¿®å¤ï¼Œå…ˆè½¬æ¢ä¸º numpy æ•°ç»„ï¼š
```python
states = th.tensor(np.array(transition_dict['states']), dtype=th.float)
```

## ğŸ“š å­¦ä¹ èµ„æº

- [PPO åŸè®ºæ–‡](https://arxiv.org/abs/1707.06347): Proximal Policy Optimization Algorithms
- [OpenAI Spinning Up](https://spinningup.openai.com/en/latest/algorithms/ppo.html): PPO æ•™ç¨‹
- [Gymnasium æ–‡æ¡£](https://gymnasium.farama.org/): ç¯å¢ƒæ–‡æ¡£

## ğŸ“ TODO

- [ ] æ·»åŠ æ¨¡å‹ä¿å­˜å’ŒåŠ è½½åŠŸèƒ½
- [ ] å®ç°å¹¶è¡Œç¯å¢ƒè®­ç»ƒ
- [ ] æ”¯æŒè¿ç»­åŠ¨ä½œç©ºé—´
- [ ] æ·»åŠ  TensorBoard æ—¥å¿—
- [ ] å®ç°å…¶ä»– RL ç®—æ³•ï¼ˆA2Cã€TRPO ç­‰ï¼‰

## ğŸ‘¨â€ğŸ’» ä½œè€…

- é¡¹ç›®åˆ›å»ºæ—¥æœŸ: 2025å¹´11æœˆ19æ—¥
- åŸºäº: PyTorch + Gymnasium

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®ä»…ä¾›å­¦ä¹ å’Œç ”ç©¶ä½¿ç”¨ã€‚

---

**Happy Reinforcement Learning! ğŸ‰**
